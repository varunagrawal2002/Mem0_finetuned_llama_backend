{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecbe046a-90b3-4fcd-a32f-6bd06b270fcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numpy 1.24.4\n",
      "Uninstalling numpy-1.24.4:\n",
      "  Successfully uninstalled numpy-1.24.4\n",
      "Found existing installation: networkx 2.4\n",
      "Uninstalling networkx-2.4:\n",
      "\u001b[31mERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/shutil.py\", line 816, in move\n",
      "    os.rename(src, real_dst)\n",
      "PermissionError: [Errno 13] Permission denied: '/usr/lib/python3/dist-packages/networkx' -> '/tmp/pip-uninstall-ouw_ql2l'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pip/_internal/cli/base_command.py\", line 107, in _run_wrapper\n",
      "    status = _inner_run()\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pip/_internal/cli/base_command.py\", line 98, in _inner_run\n",
      "    return self.run(options, args)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pip/_internal/commands/uninstall.py\", line 105, in run\n",
      "    uninstall_pathset = req.uninstall(\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pip/_internal/req/req_install.py\", line 675, in uninstall\n",
      "    uninstalled_pathset.remove(auto_confirm, verbose)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pip/_internal/req/req_uninstall.py\", line 373, in remove\n",
      "    moved.stash(path)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pip/_internal/req/req_uninstall.py\", line 264, in stash\n",
      "    renames(path, new_path)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pip/_internal/utils/misc.py\", line 342, in renames\n",
      "    shutil.move(old, new)\n",
      "  File \"/usr/lib/python3.10/shutil.py\", line 834, in move\n",
      "    rmtree(src)\n",
      "  File \"/usr/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/usr/lib/python3.10/shutil.py\", line 658, in _rmtree_safe_fd\n",
      "    _rmtree_safe_fd(dirfd, fullname, onerror)\n",
      "  File \"/usr/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/usr/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "PermissionError: [Errno 13] Permission denied: 'graphviews.py'\u001b[0m\u001b[31m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Collecting numpy<2.0,>=1.26.0\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[33mWARNING: Error parsing dependencies of flatbuffers: Invalid version: '1.12.1-git20200711.33e2d80-dfsg1-0.6'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: numpy\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "unsloth-zoo 2025.11.3 requires transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,<=4.57.2,>=4.51.3, but you have transformers 5.0.0.dev0 which is incompatible.\n",
      "unsloth-zoo 2025.11.3 requires trl!=0.19.0,<=0.24.0,>=0.18.2, but you have trl 0.14.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: networkx in /usr/lib/python3/dist-packages (2.4)\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Error parsing dependencies of flatbuffers: Invalid version: '1.12.1-git20200711.33e2d80-dfsg1-0.6'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: networkx\n",
      "Successfully installed networkx-3.4.2\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: unsloth in ./.local/lib/python3.10/site-packages (2025.11.2)\n",
      "\u001b[33mWARNING: Error parsing dependencies of flatbuffers: Invalid version: '1.12.1-git20200711.33e2d80-dfsg1-0.6'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y numpy networkx unsloth unsloth_zoo transformers\n",
    "!pip install \"numpy>=1.26.0,<2.0\"\n",
    "!pip install --upgrade networkx\n",
    "!pip install unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdfe7aff-7b9e-4804-92be-1b6912ad2102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import TrainingArguments\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b932d2b-a2c8-481b-9157-a129e2c6a57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "==((====))==  Unsloth 2025.11.2: Fast Llama patching. Transformers: 5.0.0.dev0.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.495 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model loaded\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading model...\")\n",
    "max_seq_length = 2048\n",
    "dtype = None  \n",
    "load_in_4bit = False\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Meta-Llama-3.1-8B-Instruct\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(\"âœ“ Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d34749af-f7c2-482d-8c21-c5e5249ab0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuring LoRA adapters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.11.2 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ LoRA configured\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nConfiguring LoRA adapters...\")\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, \n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = True,\n",
    "    random_state = 42,\n",
    ")\n",
    "print(\"âœ“ LoRA configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c88b30c-39b9-4de7-b8f1-2300ceb9df6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataset...\n",
      "âœ“ Loaded 5000 samples\n",
      "âœ“ Dataset loaded: 5000 examples\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoading dataset...\")\n",
    "dataset = load_dataset(\"json\", data_files=\"coqa_mctest_5000_samples.jsonl\", split=\"train\")\n",
    "print(f\"âœ“ Loaded {len(dataset)} samples\")\n",
    "\n",
    "train_test_split = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "eval_dataset = train_test_split[\"test\"]\n",
    "print(f\"âœ“ Dataset loaded: {len(dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "747cc934-863c-4abd-b7e1-5050cc7f1213",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setting up trainer...\n",
      "Training configuration:\n",
      "  - Total samples: 4500\n",
      "  - Global batch size: 8\n",
      "  - Training steps: 562\n",
      "âœ“ Trainer configured\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSetting up trainer...\")\n",
    "TOTAL_SAMPLES = len(train_dataset)\n",
    "GLOBAL_BATCH_SIZE = 2 * 4\n",
    "TRAIN_STEPS = TOTAL_SAMPLES // GLOBAL_BATCH_SIZE\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  - Total samples: {TOTAL_SAMPLES}\")\n",
    "print(f\"  - Global batch size: {GLOBAL_BATCH_SIZE}\")\n",
    "print(f\"  - Training steps: {TRAIN_STEPS}\")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    args=SFTConfig(\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=5,\n",
    "        max_steps=TRAIN_STEPS,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=42,\n",
    "        output_dir=\"outputs\",\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=100,\n",
    "        per_device_eval_batch_size=4,\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=2,\n",
    "        dataloader_num_workers=0\n",
    "    ),\n",
    ")\n",
    "print(\"âœ“ Trainer configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e84d168c-9169-4e03-b225-a4aef11a31b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting model training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 4,500 | Num Epochs = 2 | Total steps = 562\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 41,943,040 of 8,072,204,288 (0.52% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='562' max='562' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [562/562 19:02, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.297100</td>\n",
       "      <td>1.258698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.365900</td>\n",
       "      <td>0.369434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.123000</td>\n",
       "      <td>0.130474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.078300</td>\n",
       "      <td>0.087987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.086600</td>\n",
       "      <td>0.076535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Training finished!\n",
      "\n",
      "Merging LoRA adapters into base model...\n",
      "âœ“ LoRA adapters merged\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStarting model training...\")\n",
    "trainer.train()\n",
    "print(\"âœ“ Training finished!\")\n",
    "\n",
    "print(\"\\nMerging LoRA adapters into base model...\")\n",
    "model = model.merge_and_unload()\n",
    "print(\"âœ“ LoRA adapters merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17458aed-b503-4ae9-9d1d-909bb464113c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('llama-3.1-8b-finetuned/tokenizer_config.json',\n",
       " 'llama-3.1-8b-finetuned/special_tokens_map.json',\n",
       " 'llama-3.1-8b-finetuned/chat_template.jinja',\n",
       " 'llama-3.1-8b-finetuned/tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"llama-3.1-8b-finetuned\")\n",
    "tokenizer.save_pretrained(\"llama-3.1-8b-finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abffd24a-e4dc-4468-bf67-a4aab07b5a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging model weights to 16-bit format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/unsloth_zoo/saving_utils.py:969: UserWarning: Model is not a PeftModel (no Lora adapters detected). Skipping Merge. Please use save_pretrained() or push_to_hub() instead!\n",
      "  warnings.warn(\"Model is not a PeftModel (no Lora adapters detected). Skipping Merge. Please use save_pretrained() or push_to_hub() instead!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Converting to GGUF format...\n",
      "==((====))==  Unsloth: Conversion from HF to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF bf16 might take 3 minutes.\n",
      "\\        /    [2] Converting GGUF bf16 to ['q4_k_m'] might take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: llama.cpp found in the system. Skipping installation.\n",
      "Unsloth: Preparing converter script...\n",
      "Unsloth: [1] Converting model into bf16 GGUF format.\n",
      "This might take 3 minutes...\n",
      "Unsloth: Initial conversion completed! Files: ['Meta-Llama-3.1-8B-Instruct.BF16.gguf']\n",
      "Unsloth: [2] Converting GGUF bf16 into q4_k_m. This might take 10 minutes...\n",
      "Unsloth: Model files cleanup...\n",
      "Unsloth: All GGUF conversions completed successfully!\n",
      "Generated files: ['Meta-Llama-3.1-8B-Instruct.Q4_K_M.gguf']\n",
      "Unsloth: example usage for text only LLMs: llama-cli --model Meta-Llama-3.1-8B-Instruct.Q4_K_M.gguf -p \"why is the sky blue?\"\n",
      "Unsloth: Saved Ollama Modelfile to current directory\n",
      "Unsloth: convert model to ollama format by running - ollama create model_name -f ./Modelfile - inside current directory.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'save_directory': 'llama-3.1-8b-finetuned',\n",
       " 'gguf_files': ['Meta-Llama-3.1-8B-Instruct.Q4_K_M.gguf'],\n",
       " 'modelfile_location': '/home/ubuntu/Modelfile',\n",
       " 'want_full_precision': False,\n",
       " 'is_vlm': False,\n",
       " 'fix_bos_token': False}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained_gguf(\n",
    "    \"llama-3.1-8b-finetuned\",\n",
    "    tokenizer,\n",
    "    quantization_method=\"q4_k_m\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b584d96-f9e7-4b39-ac22-bbf6e7058d91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
